{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Build the discriminative model </h1>\n",
    " \n",
    "This notebook has the necesary code to create a discriminative model whose purpose is classify 11 types of food and non food.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config logs\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> SETTINGS </h4>\n",
    "Define the settings of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "OUTPUT_MODEL_FILE = \"inceptionv3-ft120_910acc.model\"\n",
    "\n",
    "IM_WIDTH, IM_HEIGHT = 299, 299  # fixed size for InceptionV3\n",
    "NB_EPOCHS_FINETUNE = 100\n",
    "NB_EPOCHS_TRANSFERLEARNING = 10\n",
    "\n",
    "BAT_SIZE = 32\n",
    "FC_SIZE = 1024\n",
    "NB_IV3_LAYERS_TO_FREEZE = 120\n",
    "\n",
    "SOURCE_PATH = 'Food12/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Prepare the data </h3>\n",
    "\n",
    "For reading from path the data, pre-process it and use data augmentation, the ImageDataGenerator method of keras is used. Its advantages are that you just hace to point out the path, and with the correct folder distribution, the data is loaded and labelled correctly.\n",
    "\n",
    "The same as the pre-processing and data augmentation, the methods to data augmentation are signed, so when the data is loaded, the selected pre-processing is developed.\n",
    "\n",
    "With Image Data Augmentation it is not necessary to be warned with the size of the dataset because Keras manage the data \"on the fly\", so images are not loaded in memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the food and non-food classes classes\n",
    "\n",
    "classes = ['Bread', 'Dairy product', 'Dessert', 'Egg', 'Fried food', 'Meat', 'Noodles/Pasta', 'Rice', 'Seafood', 'Soup',\n",
    "           'Vegetable/Fruit', 'Non food']\n",
    "\n",
    "class_to_ix = dict(zip(classes, range(len(classes))))\n",
    "ix_to_class = dict(zip(range(len(classes)), classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define the paths\n",
    "training_path = os.path.join(SOURCE_PATH, 'training/')\n",
    "validation_path = os.path.join(SOURCE_PATH, 'validation/')\n",
    "evaluation_path = os.path.join(SOURCE_PATH, 'evaluation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ImageDataGenerator to use data augmentation directly.\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 711 images belonging to 12 classes.\n",
      "Found 612 images belonging to 12 classes.\n",
      "Found 577 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# create the image generators for training, validation and testing, so the data is loaded directly from path\n",
    "# and pre-processed as have been pointed out in the ImageDataGenerator function\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_path,\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_path,\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "evaluation_generator = test_datagen.flow_from_directory(\n",
    "    evaluation_path,\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of classes\n",
    "nb_classes = train_generator.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of samples\n",
    "nb_train_samples = train_generator.samples\n",
    "nb_val_samples = validation_generator.samples\n",
    "nb_eval_samples = evaluation_generator.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create the model </h3>\n",
    "\n",
    "The selected architecture to develop the model is the InceptionV3. The model provided by keras is the one which is going to be used. \n",
    "\n",
    "Regarding with the weights, the 'imagenet' pre-trained weights are used when the model is loaded.\n",
    "\n",
    "The last layer of the model is not going to be used, because we are going to add a customized layer whose output correspond with the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bea/.virtualenvs/python3.6-keras/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False)  # include_top=False excludes final FC layer\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(FC_SIZE, activation='relu')(x)  # new FC layer, random init\n",
    "predictions = Dense(nb_classes, activation='softmax')(x)  # new softmax layer\n",
    "\n",
    "model = Model(input=base_model.input, output=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we do not want to train all the layers, just the last one, we are going to freeze the layers of the base model (the inception architecture), so just the two last denses classes are going to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that the model is defined, we need to select the optimer, the loss and if any metrics is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Prepare the training </h3>\n",
    "\n",
    "Once that orw model architecture is defined and compiled, we can start to train the model. But where are going to define, before the training to provent over-fitting, \"Early Stoppping\" method, so, when the loss at validation (bacause it has been selected) does not improve in three iterations, the training will stop directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_tl = EarlyStopping(monitor=\"val_loss\", patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train the model </h3>\n",
    "\n",
    "Here the model is trained. While is training, at the end of each epoch, the model is going to be validated with the validation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bea/.virtualenvs/python3.6-keras/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/home/bea/.virtualenvs/python3.6-keras/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., class_weight=\"auto\", callbacks=[<keras.ca..., steps_per_epoch=22, epochs=10, validation_steps=612)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/22 [===========================>..] - ETA: 4s - loss: 2.1683 - acc: 0.2925"
     ]
    }
   ],
   "source": [
    "history_tl = model.fit_generator(\n",
    "            train_generator,\n",
    "            nb_epoch=NB_EPOCHS_TRANSFERLEARNING,\n",
    "            samples_per_epoch=nb_train_samples,\n",
    "            validation_data=validation_generator,\n",
    "            nb_val_samples=nb_val_samples,\n",
    "            class_weight='auto',\n",
    "            callbacks=[early_tl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Fine tune the model </h3>\n",
    "\n",
    "When the model is trained, maybe the performance is not optimum, so some parameters could be changed and the training will improve.\n",
    "\n",
    "The parameters that we are goint to modify is:\n",
    " * Train more layers\n",
    " * Change the optimizer\n",
    " * modify the learning rate\n",
    " * apply momentum\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the layers which are not going to be trained\n",
    "for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]:\n",
    "    layer.trainable = False\n",
    "# select the one which does\n",
    "for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# compile the model with new parameters    \n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the model again with the new model parametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ft = model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    nb_epoch=NB_EPOCHS_FINETUNE,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_val_samples,\n",
    "    class_weight='auto',\n",
    "    callbacks=[early_tl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model trained and we are satisfied with its performance, we can save the model just in case we want to use it again, so it is not necessary to follow the whole procedure again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(OUTPUT_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test the model </h3>\n",
    "\n",
    "Using the testing subset, the model is evaluated, so we can know how really the performance of the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate_generator(evaluation_generator, nb_eval_samples / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The loss of the model is {score[0]}, and its accuracy is {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Test on single images </h3>\n",
    "\n",
    "When all the training and testing procedure is realized, in pointed acassions we will need that the model predict us some images.\n",
    "\n",
    "The processed image could be a image stored locally, or an url could be provided too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the input size of the model is 229, if the image has not that size, it would have to be resized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (229, 229)  # fixed size for InceptionV3 architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image or give an image url\n",
    "img_str = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image and resize if its necessary\n",
    "img = None\n",
    "if img_str.startswith('http://') or img_str.startswith('https://'):\n",
    "    response = requests.get(img_str)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "elif os.path.isfile(img_str):\n",
    "    img = Image.open(img_str)\n",
    "if img is None:\n",
    "    return None\n",
    "if img.size != target_size:\n",
    "    img = img.resize(target_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the model that we have saved during the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(OUTPUT_MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the label of the images is not the corresponding one, we create a map where we assign our label, to the keras one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx_map = {'11': 3, '10': 2, '1': 1, '0': 0, '3': 5, '2': 4, '5': 7, '4': 6, '7': 9, '6': 8, '9': 11, '8': 10}\n",
    "idx_class_map = {_v: int(_k) for _k, _v in class_idx_map.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before test the image, it is need to pre-process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the image is ready, we can feed the model with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the fact that the probabilities given by the model are represented in a vector whose length is the number of labels, we obtain the label with higher probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = preds[0]\n",
    "pred_idx = np.argmax(probs)\n",
    "pred_class = idx_class_map[pred_idx]\n",
    "pred_class_name = ix_to_class[pred_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the image classified, and we can share  it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"\\tClass: '{}'. Prob: {:.5f}\".format(pred_class_name, probs[pred_idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
